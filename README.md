# Querido DiÃ¡rio Workers

Serverless crawler for Brazilian official gazettes (diÃ¡rios oficiais) using **Cloudflare Workers** and **Queues**.

This is a TypeScript/Node.js port of the [querido-diario](https://github.com/okfn-brasil/querido-diario) project, redesigned for serverless architecture.

## Features

- âœ… **Serverless**: Runs on Cloudflare Workers (no servers to manage)
- âœ… **Scalable**: Uses Cloudflare Queues for distributed crawling
- âœ… **TypeScript**: Fully typed codebase
- âœ… **1,937 Cities**: 17 platform types implemented (**28.2% national coverage**)
- âœ… **Lightweight**: Extracts gazette metadata and PDF URLs (no file downloads)
- âœ… **Fast**: Average 400-500ms per city crawl
- âœ… **OCR Integration**: Automatic PDF text extraction with Mistral OCR API
- âœ… **Smart Caching**: KV-based deduplication to avoid reprocessing

## ğŸ“Š National Coverage

**1,573 of 5,570 Brazilian municipalities (28.24%)**

### Coverage by State

| UF | Total | Covered | Coverage | Progress |
|----|-------|---------|----------|----------|
| **MT** | 141 | 139 | **98.6%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘` |
| **PE** | 185 | 182 | **98.4%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘` |
| **RN** | 167 | 160 | **95.8%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘` |
| **CE** | 184 | 127 | **69.0%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| **MG** | 853 | 474 | **55.6%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| **RS** | 497 | 262 | **52.7%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| **PR** | 399 | 176 | **44.1%** | `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| **PI** | 224 | 31 | **13.8%** | `â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| **PB** | 223 | 22 | **9.9%** | `â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |
| Other states | 2,897 | 0 | 0.0% | `â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘` |

*Last updated: 2025-10-04*

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Cloudflare Infrastructure                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  HTTP Request â†’ Dispatcher Worker â†’ Crawl Queue â†’ Consumer Worker    â”‚
â”‚                                          â†“                             â”‚
â”‚                                    Gazettes Found                     â”‚
â”‚                                          â†“                             â”‚
â”‚                                     OCR Queue â†’ OCR Worker            â”‚
â”‚                                                    â†“                   â”‚
â”‚                                              Mistral OCR API          â”‚
â”‚                                                    â†“                   â”‚
â”‚                                              Extracted Text           â”‚
â”‚                                                    â†“                   â”‚
â”‚                                              KV Storage (optional)    â”‚
â”‚                                                                        â”‚
â”‚  1. POST /crawl with city list                                       â”‚
â”‚  2. Enqueue tasks to Cloudflare Queue                                â”‚
â”‚  3. Consumer workers process each city                                â”‚
â”‚  4. Return gazette metadata + PDF URLs                                â”‚
â”‚  5. Automatically send PDFs to OCR queue                              â”‚
â”‚  6. OCR worker processes PDFs with Mistral                            â”‚
â”‚  7. Store extracted text in KV                                        â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
querido-diario-workers/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ worker.ts                 # Main worker (dispatcher + consumer)
â”‚   â”œâ”€â”€ ocr-worker.ts             # OCR processing worker
â”‚   â”œâ”€â”€ analysis-worker.ts        # AI analysis worker
â”‚   â”œâ”€â”€ webhook-worker.ts         # Notification worker
â”‚   â”œâ”€â”€ r2-server.ts              # R2 PDF server
â”‚   â”œâ”€â”€ types/                    # TypeScript interfaces
â”‚   â”œâ”€â”€ services/                 # Core services
â”‚   â”‚   â”œâ”€â”€ mistral-ocr.ts        # Mistral OCR integration
â”‚   â”‚   â”œâ”€â”€ analysis-orchestrator.ts  # AI analysis
â”‚   â”‚   â””â”€â”€ webhook-sender.ts     # Webhook notifications
â”‚   â”œâ”€â”€ spiders/                  # Spider system
â”‚   â”‚   â”œâ”€â”€ base/                 # 23 spider implementations
â”‚   â”‚   â”œâ”€â”€ configs/              # 20 platform configs (2,792 cities)
â”‚   â”‚   â””â”€â”€ registry.ts           # Spider factory
â”‚   â”œâ”€â”€ analyzers/                # AI analysis modules
â”‚   â”œâ”€â”€ testing/                  # Automated testing system
â”‚   â””â”€â”€ utils/                    # Utilities
â”œâ”€â”€ scripts/                      # Management scripts
â”‚   â”œâ”€â”€ remote-crawl.ts           # Remote execution
â”‚   â”œâ”€â”€ find-city.ts              # City lookup
â”‚   â”œâ”€â”€ setup-concursos-webhook.ts # Webhook setup
â”‚   â”œâ”€â”€ test-city.ts              # Single city testing
â”‚   â””â”€â”€ test-platform.ts          # Platform testing
â”œâ”€â”€ wrangler*.jsonc               # Worker configurations (5)
â”œâ”€â”€ ARCHITECTURE.md               # Complete system architecture
â”œâ”€â”€ FLOW_REVIEW.md                # This document
â””â”€â”€ CITY_ID_STANDARDIZATION_PLAN.md # City naming standards
```

## Getting Started

### Prerequisites

- Node.js 18+
- npm or pnpm
- Cloudflare account (for deployment)

### Installation

```bash
npm install
```

### Development

Run the unified worker locally:

```bash
npm run dev
```

### Deployment

Deploy the unified worker:

```bash
npm run deploy
```

## API Usage

### Start a Crawl

**POST** `/crawl`

```json
{
  "cities": ["ba_acajutiba", "ba_alagoinhas"],
  "startDate": "2024-01-01",
  "endDate": "2024-12-31"
}
```

**Response:**

```json
{
  "success": true,
  "tasksEnqueued": 2,
  "cities": ["ba_acajutiba", "ba_alagoinhas"]
}
```

### List Available Spiders

**GET** `/spiders`

```json
{
  "total": 50,
  "spiders": [
    {
      "id": "ba_acajutiba",
      "name": "Acajutiba - BA",
      "territoryId": "2900306",
      "type": "doem",
      "startDate": "2013-01-30"
    }
  ]
}
```

### Filter by Type

**GET** `/spiders?type=doem`

## Supported Platforms

### Current

- **DOEM** (DiÃ¡rio Oficial EletrÃ´nico dos MunicÃ­pios): **56 cities** âœ…
  - 52 cities in Bahia (BA)
  - 1 city in Pernambuco (PE)
  - 2 cities in ParanÃ¡ (PR)
  - 1 city in Sergipe (SE)

## Supported Platforms

| Platform | Cities | Status |
|----------|--------|--------|
| **SIGPub** | 1,573 | âœ… |
| **Instar** | 111 | âœ… |
| **DOEM** | 56 | âœ… |
| **DOSP** | 42 | âœ… |
| **ADiarios V1** | 34 | âœ… |
| **MunicipioOnline** | 26 | âœ… |
| **AtendeV2** | 22 | âœ… |
| **DIOF** | 20 | âœ… |
| **DiarioOficialBR** | 10 | âœ… |
| **Siganet** | 10 | âœ… |
| **Modernizacao** | 7 | âœ… |
| **BarcoDigital** | 7 | âœ… |
| **ADiarios V2** | 5 | âš ï¸ Stub |
| **Aplus** | 4 | âœ… |
| **Dioenet** | 4 | âœ… |
| **AdministracaoPublica** | 3 | âœ… |
| **PTIO** | 3 | âœ… |
| **Total** | **1,937** | **28.2%** |

### Planned

- Other platforms: ~158 cities remaining

## OCR System

### Overview

The OCR system automatically processes PDF documents from gazettes using **Mistral OCR API** (`mistral-ocr-latest`). When gazettes are found by spiders, their PDF URLs are automatically sent to an OCR queue for text extraction.

### Features

- âœ… **Automatic Processing**: PDFs are sent to OCR queue automatically after crawling
- âœ… **Smart Caching**: Checks KV storage before processing to avoid duplicates
- âœ… **Mistral OCR**: Uses state-of-the-art `mistral-ocr-latest` model
- âœ… **Markdown Output**: Extracted text in clean markdown format
- âœ… **Batch Processing**: Processes up to 5 PDFs simultaneously
- âœ… **Error Handling**: Automatic retries and Dead Letter Queue for failures
- âœ… **Metadata Preservation**: Maintains all gazette metadata with extracted text

### Configuration

See [OCR_SYSTEM_DOCUMENTATION.md](OCR_SYSTEM_DOCUMENTATION.md) and [QUICK_START_OCR.md](QUICK_START_OCR.md) for detailed setup instructions.

**Quick setup:**

```bash
# 1. Configure Mistral API key
wrangler secret put MISTRAL_API_KEY --config wrangler-ocr.jsonc

# 2. Deploy OCR worker
npm run deploy:ocr

# 3. (Optional) Create KV namespace for caching
wrangler kv:namespace create "OCR_RESULTS"
```

### Performance

- **Processing Time**: ~700ms for simple PDFs, 2-5s per page for complex documents
- **Throughput**: 300-600 gazettes per hour
- **Limits**: 50MB max file size, 1000 pages max per document

### Cost Estimation

- **Cloudflare**: Free tier covers most usage (100k requests/day)
- **Mistral OCR**: ~$0.01 per page
- **Example**: 1,000 gazettes/day â‰ˆ $10-20/day

## Output Format

Each crawl returns gazette metadata:

```json
{
  "spiderId": "ba_acajutiba",
  "territoryId": "2900306",
  "gazettes": [
    {
      "date": "2024-01-15",
      "editionNumber": "1234",
      "fileUrl": "https://doem.org.br/ba/acajutiba/diarios/...",
      "isExtraEdition": false,
      "power": "executive_legislative",
      "territoryId": "2900306",
      "scrapedAt": "2024-10-03T16:30:00.000Z"
    }
  ],
  "stats": {
    "totalFound": 1,
    "dateRange": {
      "start": "2024-01-01",
      "end": "2024-12-31"
    },
    "requestCount": 12,
    "executionTimeMs": 3450
  }
}
```

## Development Roadmap

- [x] Core infrastructure (types, utils, base classes)
- [x] DOEM spider implementation (56 cities) âœ…
- [x] Unified worker (dispatcher + consumer)
- [x] Instar spider (111 cities) âœ…
- [x] DOSP spider (42 cities) âœ…
- [x] ADiarios V1 spider (34 cities) âœ…
- [x] DIOF spider (20 cities) âœ…
- [x] BarcoDigital spider (7 cities) âœ…
- [x] Siganet spider (10 cities) âœ…
- [x] DiarioOficialBR spider (10 cities) âœ…
- [x] Modernizacao spider (7 cities) âœ…
- [x] Aplus spider (4 cities) âœ…
- [x] Dioenet spider (4 cities) âœ…
- [x] AdministracaoPublica spider (3 cities) âœ…
- [x] PTIO spider (3 cities) âœ…
- [ ] ADiarios V2 spider (5 cities) - requires browser automation
- [ ] Remaining platforms (~158 cities)
- [ ] Storage integration (D1/KV/R2)
- [ ] Monitoring and alerting
- [ ] PDF download worker (optional)

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## License

MIT License

## Acknowledgments

Based on the original [Querido DiÃ¡rio](https://github.com/okfn-brasil/querido-diario) project by [Open Knowledge Brasil](https://ok.org.br/).

## Related Projects

- [querido-diario](https://github.com/okfn-brasil/querido-diario) - Original Python/Scrapy implementation
- [Querido DiÃ¡rio Website](https://queridodiario.ok.org.br/) - Official project website
